# 任务二
## 1.图解transformer
![image](https://user-images.githubusercontent.com/66761003/133456882-7b09ee57-82df-418d-8788-25802428773e.png)

### 编码过程
一个标准的transformer为seq2seq结构，当中包含编码器和解码器即encoder与decoder. 从输入端开始，依次经过位置编码（Positional Encoding, 用来提供词与词之间的距离信息）、
多头注意力机制（MultiHeadAttention）、层正则化（LayerNorm）、前馈网络（FFNN）和层正则化，这一阶段称之为编码过程，对于给定的输入语句将其编码为数字形式；解码过程与编码过程类似便不过多赘述。
乍一看这不就是个普通到不能再普通的神经网络吗，其实不然，它的编码器、解码器中均包含selfattention以及ffnn，用于提升对序列的信息抽取，且每个子层之间均用残差连接，输出时用层标准化，相当于将每个小型网络进行集成，多个小网络连接之后耦合成大模块即编解码器。
### 解码过程
编码器一般有多层，第一个编码器的输入是一个序列文本，最后一个编码器输出是一组序列向量，这组序列向量会作为解码器的K、V输入，其中K=V=解码器输出的序列向量表示。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中到输入序列的合适位置。
解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译），解码器当前时间步的输出又重新作为输入Q和编码器的输出K、V共同作为下一个时间步解码器的输入。然后重复这个过程，直到输出一个结束符。
解码器中的 Self Attention 层，和编码器中的 Self Attention 层的区别：

1.在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词。具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置（将attention score设置成-inf）。

2.解码器 Attention层是使用前一层的输出来构造Query 矩阵，而Key矩阵和 Value矩阵来自于编码器最终的输出。

### 输出   线性层+softmax
线性层即nn.linear, 用softmax进行输出, 这两个对于有了解深度学习的人而言还是比较熟悉的，不过多介绍. 

## 2.代码部分
既然我们已经能大概的把transformer结构给拆解出来，便开始着手进行代码实现：（emmmmmmm码实在太多，讲点个人理解吧）

### Class Transformer

init 函数
接受参数：source词汇数、target词汇数、最大句子长度、token向量维度'('符号表明参数可选，有默认值)、model维度'、内部维度'、层数'、head数'、K的维度'、V的维度、dropout比例、tgt_emb_prj_weight_sharing、emb_src_tgt_weight_sharing
方法过程：创建encoder（关联Ecoder 类） ——> 创建decoder（关联Decoder类） ——> 创建目标单词投影层（用一个dense层达成） ——> 若tgt_emb_prj_weight_sharing为真，Share the weight matrix between target word embedding & the final logit dense layer（此操作要求model维度和token向量维度一样，比如都是默认的512） ——> 若 emb_src_tgt_weight_sharing 为真，Share the weight matrix between source & target word embeddings （此操作要求source词汇数和target词汇数一样，其实从意思上来说，source和target应该是一门语言的词汇，一个词汇表。）

forward 函数
接收参数：source sequence 、source position 、target sequence、 target position
方法过程：调用encoder（关联Encoder类） ——> 调用decoder（关联Decoder类）——> 调用目标单词投影层
返回：logits

### Class Encoder

#### init 函数
接受参数：scource词汇数、最大句子长度、token向量维度、层数、head数、K维度、V维度、model维度、内部维度、dropout比例'
方法过程：创建word embedding层（输入维度是source词汇数、输出维度是token向量维度、）对source word做embedding ——> 创建position embedding层 ——> 创建layer stack (关联EncoderLayer)

#### forward 函数
接受参数：source 序列、source position 、 是否返回attention'
方法过程： 计算attention mask矩阵（关联函数get_attn_keypad_mask） ——> 计算padding的mask （关联函数get_non_pad_mask） ——> 调用word embedding层和position embedding层，并把结果加起来作为 encoder的ouput ——> 调用 layer_stack，逐层得到 encoder output， 如果 要求返回attention ，则会逐层吧 attention 加入 attn_list
返回： encoder output ，或（encoder output ，attn_list）

### Class Decoder

#### init 函数
接受参数：target 词汇 数、 最大句子长度、 token 向量维度、层数、head数、 K维度、 V维度、 model维度、 内部维度、 dropout比例
方法过程：创建target词embedding层 ——> 创建position embedding层 ——> 创建layer_stack（多个DecoderLyaer组成，关联DecoderLayer类）

#### forward函数
接受参数：target sequence、target position、source sequece 、encoder output、return_attns
方法过程：计算padding的mask（关联函数get_non_pad_mask） ——> 计算 subsquent mask（decoder 需要 遮盖未预测的位置，关联函数 get_subsequent_mask）——> 计算attention的mask（关联函数 get_attn_key_pad_mask） ——> 将attention的mask 和subsquent mask 相加——> 逐步调用layer_stack中的decoder_layer 得到dec_output，如果return_attns为真，则逐层保存dec_slf_attn和dec_enc_attn。
返回 dec_output, （dec_slf_attn_list, dec_enc_attn_list）

#### Function get_attn_key_pad_mask

接受参数：sequence_k，sequence_q
方法过程：计算sequence_k的padding mask ——> 将padding mask 由向量 扩展成矩阵，扩展第二个维度，长度为sequence_q的第二维长度（squence lenth），最后这个矩阵的维度是batch size * length of q * lenght of k。
返回：attention 的 mask矩阵

#### Function get_non_pad_mask

接受参数：sequence
方法过程：断言sequence的维度是2 ——> 判断sequence的非padding部分
返回：一个和sequence长度相同的mask tensor，padding部分为0，非padding部分为1

#### Function get_subsequent_mask

接受参数：sequence
方法过程：构造一个length of seq * length of seq 的ones 矩阵，然后转成上三角为1其余为0的矩阵
返回：返回一个上三角矩阵，维度为batch size * l_s * l_s









